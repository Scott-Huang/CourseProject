[{"title": "tr-lec1-transcription-english.vtt", "text": "This lecture is about natural language content analysis. As you see from this picture, this is really the first step to process any text data, text data in natural languages. So computers have to understand natural language to some extent in order to make use of the data. So that's the topic of this lecture. We're going to cover three things. First, what is natural language processing? Which is the main technique for processing natural language to obtain understanding? The second is the state of the art in NLP, which stands for natural language processing. Finally, we're going to cover the relation between natural language processing and text retrieval. First what is NLP? Well the best way to explain it is to think about if you see a text in a foreign language that you can understand. Now what do you have to do in order to understand that text? This is basically what computers are facing, right? So looking at the simple sentence like a dog is chasing a boy on the playground. We don't have any problem with understanding this sentence. But imagine what the computer would have to do in order to understand it, or in general it would have to do the following. First we have to know dogs are a noun chasing is a verb etc. So this is called lexical analysis or part of speech tagging. And we need to figure out the syntactic categories of those words. So that's the first step. After that, we're going to figure out the structure of the sentence. So for example, here it shows that A and a dog would go together to form a noun phrase. And we won't have dog and is to go first, and there are some structures that are not just right. But this structure shows what we might get if we look at the sentence and try to interpret the sentence. Some words would go together 1st and then they will go together with other words. So here we show we have noun phrases as intermediate components and then verbal phrases. Finally we have a sentence. And to get this structure we need to do something called a syntactic analysis or parsing, and we may have a parser. A computer program that would automatically create this structure. Now at this point you would know the structure of this sentence, but still you don't know the meaning of the sentence, so we have to go further to semantic analysis. In our mind, we usually can map such a sentence to what we already know in our knowledge base. And for example, you might imagine a dog that looks like that there's a boy and there's some activity here. But for a computer would have to use symbols to denote that, right? So we would use a symbol D1 that denote a dog and B1 to denote a boy and then P1 to denote the playground, playground. Now there is also chasing activity that's happening here, so we have a relation chasing here that connects all these symbols. So this is how computer would obtain some understanding of this sentence. Now from this representation we could also further infer some other things, and we might indeed naturally think of something else when we read the text, and this is called inference. So for example, if you believe that if someone is being chased and this person might be scared with this rule, you can see computers could also infer that this boy may be scared. So this is some extra knowledge that you would infer based on understanding of the text. You can even go further to understand why the person said this sentence, so this has reduced the use of language. This is called. Pragmatic analysis. In order to understand the speech actor of a sentence. Like we say something to basically achieve some goal. There's some purpose there, and this has to do with the use of language. In this case, the person who said this sentence might be reminding another person to bring back the dog. That could be one possible intent to reach this level of understanding would require all these steps. And a Computer would have to go through all these steps in order to completely understand this sentence. Yet we humans have no trouble with understanding that, we instantly will get everything. And there is a reason for that. That's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence. Computers unfortunately, are hard to obtain such understanding. They don't have such a knowledge base, they are still incapable of doing reasoning under uncertainties. So that makes natural language processing difficult for computers. But the fundamental reason why a natural language processing is difficult for computers is simply because natural language has not been designed for computers. They natural languages are designed for us to communicate. There are other languages designed for computers. For example program languages. Those are harder for us, so natural languages is designed to make our communication efficient. As a result, we omit a lot of common sense knowledge because we assume everyone knows about that. We also keep a lot of ambiguities because we assume the receiver or the hearer could know how to disambiguate ambiguous word based on the knowledge or the context. There's no need to invent the different words for different meanings. We could overload the same word with different meanings without the problem. Because of these reasons, this makes every step in natural language processing difficult. For computers, ambiguity is the main difficulty. And common sense reasoning is often required. That's also hard. So let me give you some examples of challenges here. Consider the word level ambiguity. The same word can have different syntactic categories. For example, design can be a noun or a verb. The word root may have multiple meanings, so square root in math sense, or the root of a plant. You might be able to think of other meanings. There are also syntactical ambiguities, for example. The main topic of this lecture, natural language processing can actually be interpreted in two ways in terms of the structure. Think for a moment to see if you can figure that out. We usually think of this as processing of natural language. But you could also think of this as you say, language processes is natural. Alright, so this is an example of syntactic ambiguity where we have different structures that can be applied to the same sequence of words. Another common example of an ambiguous sentence is the following. A man saw a boy with a telescope. Now in this case, the question is who had the telescope? Right, this is called a prepositional phrase attachment. Ambiguity, or PP attachment ambiguity. Now we generally don't have a problem with these ambiguities. Because we have a lot of background and knowledge to help us disambiguate the ambiguity. Another example of difficulties is anaphora resolution, so think about the sentence like John persuaded Bill to buy a TV for himself. The question here is does himself refer to John or Bill? So again, this is something that you have to use some background or the context to figure out. Finally, presupposition is another problem. Consider the sentence. He has quit smoking. This obviously implies that he smoked before. So imagine a computer wants to understand all these subtle differences and meanings. It would have to use a lot of knowledge to figure that out. It also would have to maintain a large knowledge knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world. So This is why it's very difficult. So as a result, we are still not perfect, in fact, far from perfect in understanding natural language using computers. So this slide sort of gives simplified view of state of the art technologies. We can do part of speech tagging pretty well, so I showed 97% accuracy here. Now this number is obviously based on a certain data set, so don't take this literally. It just shows that we can do it pretty well, but it's still not perfect. In terms of parsing, we can do partial parsing pretty well. That means we can get noun phrase structures or verbal phrases structures, or some segment of the sentence understood correctly in terms of the structure. An in some evaluation results we have seen above 90% accuracy in terms of partial parsing of sentences. Again, I have to say these numbers are relative to the data set in some other data sets. The numbers might be lower. Most of the existing work has been evaluated using news data set and so a lot of these numbers are more or less biased toward news data. Think about the social media data, the accuracy likely is lower. In terms of semantic analysis. We are far from being able to do a complete understanding of a sentence. But we have some techniques that would allow us to do partial understanding of the sentence. So I could mention some of them. For example, we have techniques that can allow us to extract the entities and relations mentioned in text articles. For example, recognizing the mentions of people, locations, organisations, etc in text. So this is called entity extraction. We may be able to recognize the relations, for example this person visited that place or this person met that person, or this company acquired another company. Such relations can be extracted by using the current natural language processing techniques. They're not perfect, but they can do well for some entities. Some entities are harder than others. We can also do word sense disambiguation to some extent. We can figure out whether this word in this sentence would have certain meaning in another context. The computer could figure out it has a different meaning. Again, it's not perfect, but you can do something in that direction. We can also do sentiment analysis, meaning to figure out the weather sentence is positive or negative. This is especially useful for review analysis, for example. So these are examples of semantic analysis and they help us to obtain partial understanding of the sentences. It's not giving us a complete understanding as I showed it before for this sentence, but it would still help us gain understanding of the content, and these can be useful. In terms of inference, we are not there yet, partly because of the general difficulty of inference and uncertainties. This is a general challenging in artificial intelligence. That's partly also because we don't have complete semantic representation for natural language text, so this is hard yet in some domains, perhaps in limited domains, when you have a lot of restrictions on the word uses, you maybe do may be able to perform inference. To some extent, but in general we cannot really do that. reliably. Speech Act analysis is also far from being done, and we can only do that analysis for various special cases. So this roughly gives you some idea about the state of the art. And then we also talk a little bit about what we can't do. And so we can't even do one hundred percent part of speech tagging. Now this looks like a simple task, but think about the example here. The two users of off may have different syntactic categories. If you try to make a fine grained distinguishing, it's not that easy to figure out such differences. It's also hard to do general, complete parsing, and again this same sentence that you saw before is example. This ambiguity can be very hard to disambiguate, and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background in order to figure out who actually had the telescope. So although the sentence looks very simple, it actually is pretty hard, and in cases when the sentence is very long. Imagine it has four or five prepositional phrases, and there are even more possibilities to figure out. It's also hard to do precise deep semantic analysis, so here's example in the sentence. John owns a restaurant. How do we define owns exactly the word own is something that we understand, but it's very hard to precisely describe the meaning of own for computers. So as a result, we have robust and general natural language processing techniques that can process a lot of text data. In a shallow way, meaning we only do superficial analysis. For example, parts of speech tagging or partial parsing or recognizing sentiment, and those are not deep understanding 'cause we're not really understanding the exact meaning of a sentence. On the other hand, the deeper understanding techniques tend not to scale up well, meaning that they would fail on some unrestricted text. And, if you don't restrict the text domain or the use of words, then these techniques tend not to work well. They may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on, but generally wouldn't work well. The data that are very different from the training data, so this pretty much summarizes the state of the art of natural language processing. Of course, within such a short amount of time, we can't really give you a complete view of NLP, which is big field an either expect to see multiple courses on natural language processing. topic itself, but because of its relevance to the topic we talk about, it's useful for you to know the background. In case you haven't been exposed to that. So what does that mean for text retrieval? In text retrieval, we're dealing with all kinds of text. It's very hard to restrict the text to a certain domain. And we also often dealing with a lot of text data. So that means the NLP techniques must be general, robust, and efficient, and that just implies today we can only use fairly shallow and NLP techniques for text retrieval. In fact, most search engines today use something called a bag of words representation. Now, this is probably the simplest representation you can possibly think of. That is to turn text data into simply a bag of words, meaning we will keep individual words, but will ignore all the orders of words. And we'll keep duplicated occurrences of words. So this is called a bag of words representation. When you represent the text in this way, you ignore a lot of other information and that just makes it harder to understand the exact meaning of a sentence, because we've lost the order. But yet this representation tends to actually work pretty well for most search tasks, and this is partly because the search task is not all that difficult. If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions. So in comparison, some other tasks, for example machine translation, would require you to understand the language accurately, otherwise the translation would be wrong. So in comparison, search task is all relatively easy. Such a representation is often sufficient, and that's also the representation that the major search engines today, like a Google or Bing or using. Of course I put in parentheses is here, but not all. Of course there are many queries that are not answered well by the current search engines and they do require a representation that would go beyond the bag of words representation that would require more natural language processing to be done. There was another reason why we have not used the sophisticated NLP techniques in modern search engines, and that's because some retrieval techniques actually naturally solve the problem of NLP. So one example is word sense disambiguation. Think about the world like Java. It could mean coffee, or could mean program language. If you look at the world alone, it would be ambiguous, but when the user uses the word in the query, usually there are other words. For example, I'm looking for usage of Java applet. When I have applet there that implies. Java Means program language. And that context can help us naturally prefer documents where Java is referring to program language 'cause those documents would probably match applet as well if Java occurs in the document in a way that it means coffee. Then you would never match applet or with very small probability, right? So this is the case when some retrieval techniques naturally achieve the goal of word sense disambiguation. Another example is. Some technical code feedback which we will talk about later in some of the lectures. This technical code would allow us to add additional words to the query and those additional words could be related to the query words. And these words can help matching documents where the original query words have not occurred. So this achieves to some extent. Semantic matching of terms. So those techniques also helped us bypass some of the difficulties in natural language processing. However, in the long run we still need deeper natural language processing techniques in order to improve the accuracy of the current search engines, and it's particularly needed for complex search tasks. Or for question answering. Google has recently launched Knowledge Graph and this is one step toward that goal. 'cause knowledge graph would contain entities and their relations, and this goes beyond the simple bag of words representation and such technique should help us improve the search engine utility significantly. Although this is still an open topic for research and exploration. In summary in this lecture we talked about what is an NLP and We've talked about the state of the art techniques, what we can do, what we cannot do, and finally, we also explain the why bag of words representation remains the dominant representation used in modern search engines, even though deeper NLP would be needed for future search engines. If you want to know more, you can take a look at some additional readings. I only cited one here and that's a good starting point. Thanks.", "segmentation": "[8, 44, 99, 159]", "keywords": "[['introduction, natural language', 'natural language processing', 'topic cover'], ['natural language processing', 'lexical analysis', 'syntactic analysis', 'pragmatic analysis', 'understanding'], ['word level ambiguity', 'syntactic ambiguity', 'difficulty'], ['speech tag parsing', 'entity extraction', 'speech act analysis', 'semantic analysis'], ['text retrieval', 'bag of words', 'search engine', 'query']]"}, {"title": "tr-lec30-transcription-english.vtt", "text": "This lecture is about the web indexing. In this lecture we will continue talking about the web search and we're going to talk about how to create a web scale index. So once we crawled the web, we've got a lot of web pages. The next step is to use the indexer to Create the inverted index. In general, we can use the standard information retrieval techniques for creating the index, and that is what we talked about in the previous lecture. But there are new challenges that we have to solve for web scale indexing and the two main challenges. Our scalability and efficiency. The index would be so large that it cannot actually fit in into any single machine or a single disk, so we have to store the data on multiple machines. Also, because the data is so large, it's beneficial to process the data in parallel so that we can produce the index quickly. Now, to address these challenges, Google has made a number of innovations. One is the Google file system that's a general distributed file system that can help programmers manage files stored on a cluster of machines. The second is MapRecuce. This is a general software framework for supporting parallel computation. Hadoop is the most known open source implementation of map reduce, now used in many applications. So this is the architecture of the Google File System. It uses very simple centralized management mechanism to manage it all the specific locations of files, so that maintains the file name, space and look up a table to know where exactly each file is stored. The application client would then talk to this GFS master and then obtain specific locations of the files that they want to process. And once the GFS client obtained the. The specific information about the files, then the application client can talk to the specific servers where the data actually sit directly so that you can avoid involving other nodes in the network. So when this file system stores the files on machines the system also would create a fixed size of chunks so that data files are separated into many chunks. Each chunk is 64 MB, so it's pretty big, and that's a property for large data processing. These chunks are replicated to ensure reliability, so this is something that the programmer doesn't have to worry about. It's all taken care of by this fire system, so from the application perspective, the programmer would see this as if it's a normal file. The program doesn't have to know where exactly it's stored and can just invoke high level operators to process the file. (And) Another feature is that the data transfers directly between application and chunk servers, so it's efficient in this sense. On top of the Google File System and Google also proposed map reduce as a general framework for parallel programming. Now this is very useful to support a task like a building inverted index. So this framework is hiding a lot of low level features from the program. As a result, the programmer can make a minimum effort to create a application that can be run on large cluster in parallel. And so some of the low level details hidden in the framework, including the specific network communications or load balancing or where the tasks are executed. All these details are hidden from the programmer. There is also a nice feature which is the built-in fault tolerance. If one server is broken, let's say service down and then some tasks may not be finished, then the map reduce mechanism would know that the task has not been done, so automatically dispatches the task on other servers that can do the job and therefore again the program doesn't have to worry about that. So here's how MapReduce works. The input data will be separated into a number of key value pairs. Now, what exactly is in the value will depend on the data, and it's actually a fairly general framework to allow you to just partition the data into different parts, and each part can be then processed in parallel. Each key value pair would be send to a map function. The programmer would write map function of course. And then the map function would then process this key value pair and would generator a number of other key value pairs. Of course the new key is usually different from the old key that's given to the map as input. And these key value pairs are the output of the map function and all the outputs of all the map functions would be then collected. And then there will be for the sorted based on the key, and the result is that all the values that are associated with the same key would be then grouped together. So now we've got a pair of a key and a set of values that are attached to this key. So this will then be sent to a Reduce function. Now of course, each Reduce function will handle a different key, so we will send these output values to multiple, reduce functions, each handling unique key. A reduce function would then process the input. Which is a key and a set of values to produce another set of key values as the output. So these output values will be then collected together to form the final output. Right, so this is the general framework of MapReduce. Now the programmer only needs to write the Map function and the Reduce function. Everything else is actually taken care of by the MapReduce framework. So you can see the program really only needs to do minimum work. And with such a framework the input data can be partitioned into multiple parts, each is processed in parallel, first by map, and then in the process after we reach the reduce stage, then multiple reduce functions can also further process the different keys and their associated values in parallel, so it achieves (some) It achieves the purpose of parallel processing of large data set. So let's take a look at a simple example and that's what accounting. How the input is files containing words. And the output that we want to generate is the number of occurrences of each word, so it's the word account. We know this kind of counting would be useful to, for example, assess the popularity of a word in a large collection, and this is useful for achieving effect of IDF weiging. Or search. So how can we solve this problem? One natural thought is that. This task can be done in parallel by simply counting different parts of the file in parallel, and then in the end we just combine all the counts, and that's precisely the idea of what we can do with MapReduce. We can parallelize on lines in this input file. So more specifically, we can assume the input to each map function is key value pair that represents the line number and the stream on that line. So the first line, for example, has the key of 1 and the value is \"Hello World\" \"Bye World\" and just 4 words. On that line so this key value pair will be sent to a map function. The map function would then just count the words in this line, and in this case of course there are only four words. Each word gets a count of one, and these are the output that you see here on this slide. From this map function. So the map function is really very simple if you look at the what the pseudocode looks like on the right side you see it simply needs to iterate over all the words in this line and then just call the collect function, which means it would then send the world and the counter to the collector. The collector would then try to sort all these. key value pairs from different Map functions, so the function is very simple and the programmer specifies this function as a way to process each part of the data. Of course, the second line will be handled by a different map function, which will produce a similar output. OK, now the output from the map functions will be then send to a collector and the collector will do the internal grouping or sorting. So at this stage you can see we have collected multiple pairs, each pair is a word and its count in the line. So once we see all these pairs then we can sort them based on the key which is the word. So we will collect all the counts of a word like a \"Bye\" here together. An similarly we do that for other words like \"Hadoop\", \"Hello\" etc. So each world now is attached to a number of values, a number of counts. And these counts represent the occurrences of this word in different lines. So now we have got a new pair of a key and a set of values and this pair will then be feeding to reduce function. So the reduce function now would have to finish the job of counting the total occurrences of this word. Now it has already got all these partial accounts, so all it needs to do is similarly to add them up so the reduce function shown here is very simple as well. You have a counter and then iterate over all the words that you see in this array, and then you just accumulated the count. And then finally output the key and the total count, and that's precisely what we want as the output of this whole program. So you can see this is already very similar to building an inverted index, and if you think about it, the output here is indexed by world and we have already got the dictionary. Basically we have got the counts, but what's missing is the document IDs and the specific frequency counts of words in those documents, so we can modify this slightly to actually build inverted index in parallel. So here's one way to do that. So in this case we can assume the input to map function is a pair of a key, which denotes the document ID and the value denoting the stream for that document. So it's all the words in that document, and so the map function will do something very similar to what we have seen in the word count example. It simply groups all the counts of this word in this document together and it would then generate the set of key value pairs. Each key is a word. And the value is the count of this orld in this document, plus the document ID. Now you can easily see why we need to add document ID here. Of course later in the inverted index we would like to keep this information so the map function should keep track of it and this can be sent to the reduce function later. Similarly, another document D2 can be processed in the same way, so in the end that again there was a sorting mechanism that would group them together and then we will have just a key like \"Java\" associated with all the documents that match this key or the documents where \"Java\" occurred. And the counts. So the counts of Java in those documents. And this will be collected together and this will be also fed into the reduce function. So now you can see the reduce function has already got input that looks like a inverted index entry, right? So it's just the word and all the documents that contain the word and the frequencies of the word in those documents. So all it needs to do is simply to concatenate them into a continuous chunk of data, and this can be then written into a file system. So basically the reduce function is going to do very minimum work. So this is pseudocode for inverted index construction. Here we see two functions. Procedure Map and procedure Reduce. And a program with the specify these two functions to program on top of map reduce and you can see basically they are doing what I just described. In the case of map, it's going to count the occurrences of word using associative array and will output the old accounts together with the document ID here. So this is the reduce function On the other hand, simply concatenates all the input that it has been given and then put them together as one single entry for this key. So this is a very simple MapReduce function, yet it would allow us to construct the inverted index at very large scale and the data can be processed by different machines. The program doesn't have to take care of the details. So this is how we can do parallel index construction for web search. So to summarize, web scale indexing requires some new techniques that go beyond the standard traditional indexing techniques, mainly will have to store the index on multiple machines, and this is usually done by using file system like a Google File System, a distributed file system. And secondly, it requires creating the index in parallel because it's so large it takes a long time to create the index for all the documents. So if we can do it in parallel it will be much faster and this is done by using the MapReduce framework. Note that the post the GFS and MapReduce frameworks are very general so they can also support many other applications.", "segmentation": "[9, 25, 56]", "keywords": "[['search', 'scalability', 'inverted index'], ['Google', 'distributed file system', 'parallel computation', 'Google File System'], ['map reduce', 'Google', 'dictionary', 'map function', 'reduce function'], ['example', 'map function', 'reduce function', 'parallel', 'index']]"}, {"title": "tr-lec36-transcription-english.vtt", "text": "There are many more advanced learning algorithms, then the regression based approaches and they generally attempt to direct the optimizer retrieval measure. like MAP or nDCG. Note that the optimization objective function that we have seen on the previous slide is not directly related to retrieval measure By maximizing the prediction of one or zero, we don't necessarily optimize the ranking of those documents. One can imagine that why our prediction may not be too bad. Let's say both are around .5, so it's kinda of in the middle of zero and one for the two documents. But the ranking can be wrong, so we might have got a larger value for D2 and than D1. So that won't be good from retrieval perspective, even though by likelihood function is not bad. In contrast, we might have another case where we predicted values all around .9, let's say and by the objective functioning the error will be larger, but if we can get the order of the two documents correct, that's actually a better result. So these new, more advanced approaches will try to correct that problem. Of course, then the challenge is that the optimization problem would be harder to solve and then researchers have proposed many solutions to the problem and you can read more reference at the end to know more about the these approaches. Now these learning to rank approaches are actually general, so they can also be applied to many other ranking problems, not just the retrieval problem. So here I list some, for example recommender systems, computational advertising or summarization and there are many others that you can probably encounter in your applications. To summarize this lecture we have talked about the using machine learning to combine multiple features to improve ranking results. Actually the use of machine learning in information retrieval has started since many decades ago. So, for example, the Rocchio feedback approaches that we talked about earlier was machine learning approach applied to relevance feedback But the most recent use of machine learning has been driven by some changes in the environment of applications of retrieval systems, and first, it's mostly driven by the availability of a lot of training data in the form of click throughs. Such data won't available before, so the data can provide a lot of useful knowledge about the relevance and machine learning methods can be applied to leverage this. Secondly, it's also driven by the need for combining many features and this is not only just because there are more features available on the web that can be naturally used to improve scoring, it's also because by combining them we can improve the robustness of ranking, so this is desired for combating spams. Modern search engines are all used, some kind of machine learning techniques combined with many features to optimize ranking and this is a major feature of these commercial engines such as Google or Bing. The topic of learning to rank is still active research topic in the community and so you can expect to see new results being developed in the next few years, perhaps. Here are some additional readings that can give you more information about how learning to rank works and also some advanced methods.", "segmentation": "[12, 17]", "keywords": "[['regression based approaches', 'optimization objective function', 'ranking', 'advanced learning algorithms'], ['features', 'machine learning', 'information retrieval'], ['combining features', 'search engines', 'ranking']]"}, {"title": "tm-lec5-transcription-english.vtt", "text": "This lecture is about the text representation. In this lecture we're going to discuss text representation. And discuss how natural language processing can allow us to represent text in many different ways. Let's take a look at this example sentence again. We can represent this sentence in many different ways. 1st. We can always represent such a sentence as a string of characters. This is true for all the languages when we store them in the computer. When we store a natural language sentence as a string of characters, we have perhaps the most general way of representing text, since we can always use this approach to represent any text data. But unfortunately, using such a representation would not help us do semantic analysis, which is often needed for many applications of text mining. The reason is because we're not even recognizing words. So as a string we're going to keep all the spaces and these ASCII symbols. We can perhaps count how... what's the most frequent character in English text, or the correlation between those characters, but we can't really analyze semantics. Yet this is the most general way of representing text, because we can use this to represent any natural language text. If we try to do a little bit more natural language processing by doing word segmentation. Then we can obtain a representation of the same text, but in the form of a sequence of words. So here we see that we can identify words like: a, dog, is, chasing, etc. Now with this level of representation, we certainly can do a lot of things, and this is mainly because words are the basic units of human communication in natural language, so they are very powerful. By identifying words we can, for example, easily count what are the most frequent words in this document or in the whole collection, etc. And these words can be used to form topics. When we combine related words together and some words are positive, some words are negative, so we can also do sentiment analysis. So representing text data as a sequence of words opens up a lot of interesting analysis possibilities. However, this level of representation is slightly less general than string of characters, because in some languages such as Chinese, it's actually not that easy to identify all the word boundaries, because in such a language you see text as a sequence of characters with no space in between. So you have to rely on some special techniques to identify words. In such a language, of course, then we might make mistakes in segmenting words. So the sequence of words representation is not as robust as string of characters. But in English it's very easy to obtain this level of representation, so we can do that all the time. Now if we go further to do natural language processing, we can add a part of speech tags. Now, once we do that, we can count for example, the most frequent nouns or what kind of nouns are associated with what kind of verbs, etc. So this opens up a little bit more interesting opportunities for further analysis. Note that I use the plus sign here, because by representing text as a sequence of part of speech tags. We don't necessarily replace the original word sequence representation Instead, we add this as an additional way of representing text data, so that now the data is represented as both a sequence of words, and a sequence of part of speech tags. This enriches the representation of text data and thus, also, enables a more interesting analysis. If we go further then we'll be parsing the sentence to obtain a syntactic structure. Now this of course further open up more interesting analysis of, for example, the writing styles, or correcting grammar mistakes. If we could go further for semantic analysis, then we might be able to recognize dog as animal and we also can recognize boy as a person and playground as a location. And we can further analyze their relations, for example, dog is chasing the boy and the boy is on the playground. Now this is to add more entities and relations through entity-relation recognition. At this level, then we can do even more interesting things. For example, now we can count easily the most frequent person that's mentioned in this whole collection of news articles, or whenever you mention this person, you also tend to see mention of another person, etc. So this is very useful representation an it's also related to the Knowledge Graph that some of you may have heard of. That Google is doing as a more semantic way of representing text data. However, it's also less robust than sequence of words or even syntactic analysis, because it's not always easy to identify all the entities with the right types, and we might make mistakes, and relations are even harder to find and we might make mistakes. So this makes this level of representation less robust, yet it's very useful. Now if we move further to logical representation then we can have predicates and even inference rules. And with inference rules we can infer interesting, derived facts from the text. So that's very useful, but unfortunately at this level of representation it's even less robust and we can make mistakes, and we can't do that all the time for all kinds of sentences. And finally, speech acts with added yet another level of representation of the intent of saying this sentence. So in this case it might be a request. So knowing that would allow us to analyze more, even more interesting things about the observer order. Author of this sentence, what's the intention of saying that? What scenarios, what kind of actions will be made? So this is... Another level of analysis that would be very interesting. So this picture shows that if we move down, we generally see more sophisticated natural language processing techniques to be used. And unfortunately, such techniques would require more human effort. And they are less accurate. That means there are mistakes. So if we analyze text data at the levels that are represented, deeper analysis of language, then we have to tolerate the errors. So that also means it's still necessary to combine such deep analysis with shallow analysis based on, for example sequence of words. On the right side you see the arrow points down, to indicate that as we go down with our representation of text, it's closer to knowledge representation in our mind, and need for solving a lot of problems. Now, this is desirable because as we can represent text at the level of knowledge, we can easily extract the knowledge. That's the purpose of text mining. So there is a trade off here between doing deeper analysis that might have errors, but would give us direct knowledge that can be extracted from text and doing shallow analysis, which is more robust. But wouldn't actually give us the necessary deeper representation of knowledge. I should also say that text data are generated by humans and are meant to be consumed by humans, so as a result in a text data analysis text mining, humans play a very important role. They are always in the loop. Meaning that we should optimize the collaboration of humans and computers. So in that sense, it's OK that computers may not be able to have completely accurate representation of text data and patterns that are extracted from text data can be interpreted by humans, and humans can guide the computers to do more accurate analysis by annotating more data by providing features to guide the machine learning programs to make them work more effectively.", "segmentation": "[4, 13, 28, 47, 62]", "keywords": "[['text representation', 'example sentence'], ['string representation', 'text representation'], ['word segmentation', 'sequence of words', 'topics'], ['part-of-speech tags', 'syntactic structure', 'semantic analysis', 'knowledge graph'], ['logical representation', 'speech acts', 'level of analysis', 'mistakes'], ['trade off', 'deep analysis', 'shallow analysis', 'optimize']]"}, {"title": "tr-lec25-transcription-english.vtt", "text": "So let's plug in these smoothing methods into the ranking function to see what we will get. So, this is a general smoothing... sorry, general ranking function for smoothing with collection language model. You have seen this before. And now we have a very specific smoothing method, the JM smoothing method. So now let's see what what's the value for alpha sub D here. And, what's the value for P sub seen here? So we may need to decide this in order to figure out the exact form of the ranking function, and we also need to figure out, of course, Alpha. So let's see, well, this ratio. Is basically this right? So Here this is the probability of seeing word on the top. And this is the probability of unseen word, or in other words, lambda is basically the alpha here. So it's easy to see that this can be rewritten as this. Very simple. So we can plug this into here. And then here, what's the value for alpha? What do you think? It will be just Lambda, right? And, what would happen if we plug in this value here? If this is lambda, what can we say about this? Does it depend on the document? No, so it can be ignored. Right? So we end up having this ranking function shown here. And in this case, you can easily see this is precisely a vector space model, because this part is the sum over all the matched query terms. This is the element of the query vector what do you think is the element of the document vector? It's this, so that's our document. vector element. And let's further examine what's inside this logarithm. So one plus this, so it's going to be a non-negative log of this. It's going to be at least one, right? And this is a parameter. So Lambda is parameter and let's look and this is a TF. Now we see very clearly this TF weighting here. And. The larger the count is, the higher the weight will be. We also see IDF weighting which is given by this. And with our document length normalization here. So all these heuristics are captured in this formula. What's interesting that we kind of have got this weighting function automatically by making various assumptions, whereas in the vector space model we had to go through those heuristic design in order to get this. And in this case note that there is a specific form and we can see whether this form actually makes sense. So what do you think Is the denominator here? This is the length of document, total number of words multiplied by the probability of the word given by the collection. So this actually can be interpreted as expected count of the word. If we're going to draw a word from the collection language model and we want to draw as many as the number of words in the document. If you do that, the expected count of a word W would be precisely given by this denominator. So this ratio basically is comparing the actual count here. The actual count of the word in the document with the expected count given by this product. If the word is in fact the following, the distribution in the collection this. And if this counter is larger than the expected count, this part, this ratio would be larger than one. So that's actually a very interesting interpretation, right? It's very natural. And intuitively it makes a lot of sense. And this is one advantage of using this kind of probabilistic reasoning. Where we have made explicit assumptions, and we know precisely why we have a logarithm here and why we have these probabilities here. And we also have a formula that intuitively makes a lot of sense. And does TF-IDF weighting and document length normalization. Let's look at the Dirichlet Prior Smoothing. It's very similar to the case of JM smoothing. In this case, the smoothing parameter is Mu and that's different from lambda that we saw before, but the format looks very similar. The form of the function looks very similar. So we still have linear interpolation here. And when we compute this ratio, while we defined that is that the ratio is equal to this. But what's interesting here is that we are doing another comparison here now. We're comparing the actual count with the expected count of the word if we sample Mu words according to the collection of the probability. So note that it's interesting we don't even see document length here. Unlike in the JM smoothing. So this of course should be plugged into this part. So you might wonder, where is the document length? Interestingly, the document length is here. In alpha sub d so this would be plugged into this part. As a result, what we get is the following function here, and this is again a sum over all the matched query words. And we again see the query term frequency here. And you can interpret this as the element of a document vector. But this is no longer a simple dot product, right? Because we have this part. And note that n is the length of the query. So that just means if we score this function we have to take a sum over all the query words and then do some adjustment of the score based on the document. But it's still It's still clear that it does document length normalization because this lens is in the denominator, so a longer document will have a lower weight here. And we can also see it has TF here and then IDF. Only that this time the form of the formula is different from the previous one in JM smoothing. But intuitively, is still implements TF IDF weighting and document length normalization. Again, the form of the function is dictated by the probabilistic reasoning and assumptions that we have made. Now there are also disadvantages of this approach, and that is there's no guarantee that such a form of the formula would actually work well. So if you look back at this retrieval function. Although it's TF IDF weighting and stopping the length normalization , for example, it's unclear whether we have sub-linear transformation. But Fortunately we can see here. There is a logarithm function here, so we do have also the here, right? So we do have the sub-linear transformation, but we did not intentionally do that. That means there's no guarantee that will end up in this in this way. Suppose we don't have logarithm, then there's no sub linear transformation. As we discussed before, perhaps the formula is not going to work so well. So that's example of the gap between formal model like this and the relevance that we have to model, which is really a subjective machine. That is tight to users. So it doesn't mean we cannot fix this. For example, imagine if we did not have this logarithm, right? So we can heuristically add one, or we can even add a double logarithm, but then it would mean that the function is no longer probabilistic model. So the consequence of the modification is no longer as predictable as what we have been doing now. So that's also why, for example, BM 25 remains very competitive and still open challenge how to use probabilistic model to derive a better model than BM25. In particular, how do we use query likelihood to derive a model that would work consistently better than BM25? Currently we still cannot do that. It's still an interesting open question. So to summarize this part we've talked about the two smoothing methods. Jelinek-Mercer, which is doing fixed coefficient linear interpolation. Dirichlet Prior, this is to add pseudocounts to every word and is doing adaptive interpolation in that the coefficient would be larger for shorter documents. In both cases we can see by using these smoothing methods we would be able to reach a retrieval function, whether assumptions are clearly articulated, so they're less heuristic. Experiment results also show that these retrieval functions also are very effective, and they are comparable to BM 25 or pivoted length normalization. So this is a major advantage of probabilistic model where we don't have to do a lot of heuristic design. Yet in the end, we naturally implemented TF IDF weighting and document length normalization. Each of these functions also has precisely one smoothing parameter. In this case, of course, we still need to set the smoothing parameter, but there are also methods that can be used to estimate these parameters. So overall this shows by using probabilistic model we follow very different strategy than the vector space model. Yet in the end we end up with some retrieval functions that look very similar to a vector space model with some advantages in having assumptions clearly stated. And then the form dictated by probabilistic model. Now, this also concludes our discussion of the query likelihood problems model. And let's recall what assumptions we have made in order to derive the functions that we have seen in this lecture. We basically have made four assumptions that I listed here. The first assumption is that the relevance can be modeled by the query likelihood and the second assumption we've made. It is a query words are generated independently that allows us to decompose the probability of the whole query. Into a product of probabilities of all the words in the query. And then the third assumption that we have made is if a word is not seen in the document that we're going to let its probability with proportional to its probability in the collection of the smoothing with the collection language model, and finally we've made one of these two assumptions about the smoothing. So we either use JM smoothing or the Dirichlet smoothing. If we make these four assumptions, then we have no choice but to take the form of the retrieval function that we have seen earlier. Fortunately, the function has a nice property in that implements TF IDF weighting and documents length normalization And, these functions also work very well, so in that sense these functions are less heuristic compared with a vector space model. And there are many extensions. This basic model and you can find the discussion of them in the reference at the end of this Lecture.", "segmentation": "[23, 43, 59, 85, 107]", "keywords": "[['smoothing methods', 'JM smoothing method', 'alpha'], ['vector space model', 'TF weighting', 'IDF weighting', 'heuristics'], ['denominator', 'expected word count', 'actual word count', 'probabilistic reasoning'], ['Dirichlet prior smoothing', 'mu', 'comparison', 'document length', 'TF weighting', 'IDF weighting'], ['probabilistic reasoning', 'logarithm', 'formal model', 'BM25 model'], ['summarize', 'smoothing methods', 'probabilistic model', 'four assumptions']]"}, {"title": "tm-lec12-transcription-english.vtt", "text": "This lecture is about the syntagmatic relation discovery and mutual information. In this lecture, we're going to continue discussing syntagmatic relation discovery. In particular, we're going to talk about another concept, the information theory, called mutual information. And how it can be used to discover syntagmatic relations? Before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. So now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make. a more comparable across different pairs. In particular, mutual information, denoted by I(X;Y), measures the entropy reduction of X obtained from knowing Y. More specifically the question we're interested in here, is how much reduction in the entropy of X can we obtain by knowing Y. So mathematically, it can be defined as the difference between the original entropy of X and the conditional entropy of X given Y. And you might see here you can see here. It can also be defined as a reduction of entropy of Y, because of knowing X. Normally the two conditional entropies H(X|Y) and H(Y|X) are not equal. But interestingly, the reduction of entropy by knowing one of them is actually equal, so this quantity is called mutual information denoted by I here and this function has some interesting properties. First, it's also non negative. This is easy to understand becausw the original entropy is always not going to be lower than the possibly reduced conditional entropy. In other words, the conditional entropy would never exceed the original entropy. Knowing some information can always help us potentially, but won't hurt us in predicting X. The second property is that it's symmetric while conditional entropy is not symmetrical. Mutual information is. The third property is that it reaches its minimum zero if and only if the two random variables are completely independent. That means knowing one of them doesn't tell us anything about the other. And this last property can be verified by simply looking at the equation above. And it reaches 0 if and only if the conditional entropy of X given Y is exactly the same as original entropy of X. So that means knowing why did not help at all, and that's when X&amp;Y are completely independent. Now when we fix X to rank different Ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here H of X is fixed because X is fixed. So ranking based on mutual information is exactly the same as ranking based on the conditional entropy of X given Y. But the mutual information allows us to compare different pairs of X&amp;Y, so that's why mutual information is more general and in general more useful. So let's examine them intuition of using mutual information for syntagmatic relation mining. Now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? So this question can be framed as a mutual information question, that is, which was have higher mutual information with eats. So we're going to compute the mutual information between eats and other words. And if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. We have lower mutual information, so this I give some example here. The mutual information between eats and meats, which is the same as between meats and eats cause major information is symmetric is expected to be higher than The mutual information between eats and the. Because knowing the doesn't really help us predict eats. Similarly knowing eats doesn't help us predicting the as well. And you also can easily see that the mutual information between a word and itself is the largest which is equal to the mutual info. The entropy of this word. So because in this case the reduction is maximum because knowing one would allow us to predict the other completely so the conditional entropy is zero. Therefore the mutual information reaches its maximum. It's going to be larger than or equal to the mutual information between eats and another word. In other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself. So now let's think about how to compute the mutual information. Now, in order to do that, we often. use a different form of mutual information, and we can mathematically write the mutual information into the form shown on this slide, where we essentially see a formula that computes what's called KL-divergences or callback labeler divergance. This is another term in information theory that measures the divergance between two distributions. Now if you look at the formula, it's also sum over many combinations of different values of the two random variables, but inside the sum mainly we're doing a comparison between 2 joint distributions. The numerator has the joint actual observed. Join the distribution of the two random variables. The bottom part of the denominator can be interpreted as the expected joint distribution of the two random variables. If there were independent. Because when two random variables are independent, they joined distribution is equal to the product of the two probabilities. So this comparison would tell us whether the two variables are indeed independent if there indeed independent, then we would expect that the two are the same. But if the numerator is different from the denominator, that would mean the two variables are not independent, and that helps measure the association. The sum is simply to take into consideration of all the combinations of the values of these two random variables. In our case, each random variable can choose one of the two values 0 or 1, so we have four combinations here. So if we look at this form of mutual information it shows that the mutual information measures the diversions of the actual joint distribution from the expected distribution under the independence assumption. The larger this divergence is, the higher the mutual information would be. So now let's further look at the what are exactly the probabilities involved in this formula of mutual information. And here I listed all the probabilities involved and it's easy for you to verify that basically we have first 2 probabilities corresponding to the presence or absence of each word. So for W1, we have two probabilities shown here. They should sum to 1 because a word can either be present or absent in the segment. And similarly for the second word, we also have two probabilities representing presence or absence of this word, and this sums to one as well. And then finally we have a lot of joint probabilities that represented the scenarios of Co-occurrences of the two words. And they are shown here. Right, so this sums to 1 because the two words can only have these four possible scenarios. Either they both occur. So in that case both variables will have a value of one or one of them occurs. There are two scenarios. In these two cases, one of the random variables will be equal to 1 and the other would be 0. And finally we have the scenario when none of them occurs. So this is when the two variables taking a value of 0. And they're summing up to 1, so these are the probabilities involved in the calculation of mutual information. here. Once we know how to calculate these probabilities, we can easily calculate the mutual information. It's also interesting to note that there are some relations or constraints among these probabilities, and we already saw two of them, so the in the previous slide that you have seen that the marginal probabilities of these words sum to one, and we also have seen this constraint that says the two words can only have these four different scenarios of Co occurrences, but we also have some additional constraints listed in the bottom. And so, for example, this one means if we add up the probabilities that we observe the two words occur together and the probabilities when the word the first word occurs and the second word doesn't occur, we get exactly the probability that the first word is observed. In other words, and when the word is observed when the first word is observed and there are only two scenarios depending on weather second word is also observed. So this probability captures the first scenario when the signal word actually is also observed. And this captures the second scenario when the seond word is not observed, so we only see the first word. And it's easy to see the other equations also follow the same reasoning. Now these equations allow us to compute some probabilities based on other probabilities. And this can simplify the computation. So more specifically, and if we know the probability that a word is present, and in this case right? So if we know this. And if we know the presence of the probability of presence of the second word, then we can easily compute their absence probability, right? It's very easy to use this equation to do that. An so we this will take care of the computation of these probabilities of presence or absence of each word. Now let's look at their joint distribution, right? Let's assume that we also have available probability that they occur together. Now it's easy to see that we can actually compute the all the rest of these probabilities based on these. Specifically, for example, using this equation, we can compute the probability that the first word occurred and the second word did not, because we know these probabilities in the boxes. And similarly, using this equation we can compute the probability that we observe only the second word. And then finally we. This probability can be calculated by using this equation, because now this is known and this is also known and this is already known right? So this can be easier to calculate. Right, so now this can be calculated. So this slide shows that we only need to know how to compute these three probabilities that are shown in the boxes, namely the presence of each word and the Co occurrence of both words in a segment.", "segmentation": "[10, 25, 40, 74]", "keywords": "[['mutual information', 'entropy reduction', 'conditional entropy'], ['properties', 'mutual information', 'symmetric', 'non negative'], ['mutual information', 'syntagmatic relation mining', 'predict'], ['compute', 'mutual information', 'distribution divergance', 'independent', 'probabilities', 'random variables'], ['probabilities', 'simplify', 'compute']]"}]